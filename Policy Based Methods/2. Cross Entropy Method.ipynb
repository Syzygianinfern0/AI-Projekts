{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEM\n",
    "Unlike steepest ascent, CEM is not very prone to get stuck in local maxima. Instead of just proceeding in the direction of steepest ascent, we take the top performers and continue with them. This gives us a better chance of finding the better gradient. Also, unlike last time, I plan to use a neural network to handle the environment. We will be using the Pendulum-v0 environment for this time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HillClimber:\n",
    "    def __init__(self, env):\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.n_observations = env.observation_space.shape[0]\n",
    "        self.network = self._make_model()\n",
    "        self.best_reward = -1\n",
    "        self.noise = 0.5\n",
    "        self.noise_max = 2\n",
    "        self.noise_min = 0.001\n",
    "        self.gamma = 0.98\n",
    "    \n",
    "    def _make_model(self):\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(24, \n",
    "                                  input_dim=self.n_observations, \n",
    "                                  activation='relu'),\n",
    "            tf.keras.layers.Dense(self.n_actions_,\n",
    "                                 activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer='adam',\n",
    "                     loss='mse')\n",
    "    \n",
    "    def get_action(self, state, policy=None):\n",
    "        if policy is None:\n",
    "            actions = np.dot(state, self.policy)\n",
    "        else:\n",
    "            actions = np.dot(state, policy)\n",
    "        actions = np.exp(actions)\n",
    "        actions = actions / np.sum(actions)\n",
    "        return np.argmax(actions)\n",
    "            \n",
    "    def _noise_adder(self, better):\n",
    "        if better:\n",
    "            self.policy += np.random.normal(\n",
    "                loc=(self.noise_min + self.noise)/2,\n",
    "                scale=(self.noise - self.noise_min),\n",
    "                size=(self.n_observations, self.n_actions)\n",
    "            )\n",
    "        else:\n",
    "            self.policy = self.best_policy + np.random.normal(\n",
    "                loc=(self.noise_min + self.noise)/2,\n",
    "                scale=(self.noise - self.noise_min),\n",
    "                size=(self.n_observations, self.n_actions)\n",
    "            )\n",
    "            \n",
    "    def learn(self, discounted_reward):\n",
    "        if discounted_reward > self.best_reward:\n",
    "            self.best_reward = discounted_reward\n",
    "            self.best_policy = self.policy\n",
    "            if self.noise > self.noise_min:\n",
    "                self._noise_adder(better=True)\n",
    "                self.noise = max(self.noise_min, self.noise/2)\n",
    "        else:\n",
    "            if self.noise < self.noise_max:\n",
    "                self._noise_adder(better=False)\n",
    "                self.noise = min(self.noise_max, self.noise*1.1)\n",
    "        \n",
    "    def try_policy(self, env, evaluate=False):\n",
    "        discounted_reward = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, (1,self.n_observations))\n",
    "        done = False\n",
    "        time_steps = 0\n",
    "        while not done:\n",
    "            time_steps += 1\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if not evaluate:\n",
    "                discounted_reward += (self.gamma ** time_steps) * reward\n",
    "            else:\n",
    "                discounted_reward += reward\n",
    "                env.render()\n",
    "            state = next_state\n",
    "            state = np.reshape(state, (1,self.n_observations))\n",
    "        return discounted_reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
