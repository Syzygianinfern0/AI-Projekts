{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Steepest Hill Climb\n",
    "It refers to moving in the direction of steepest ascent of the reward-policy function. The \n",
    "method is not very good though as it may tend to get stuck in local maximas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "from collections import deque\n",
    "%matplotlib inline "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Agent class\n",
    "We will work on the cartpole environment as its good for 1st time experimentations. The policy is\n",
    "going to be a weights matrix which ouputs propabilities for each action. The propabilities \n",
    "will be generated using a $softmax$ function. Its given by \n",
    "\n",
    "![softmax](https://wikimedia.org/api/rest_v1/media/math/render/svg/02a859ba32ab892a2cdbfdafcd5a8f56e49e3d1c)\n",
    "\n",
    "Its normaly used in classification problems and hence is also suitable for this one"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class HillClimber:\n",
    "    def __init__(self, env):\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.n_observations = env.observation_space.shape[0]\n",
    "        self.policy = np.random.rand(self.n_observations, self.n_actions)\n",
    "        self.best_policy = np.copy(self.policy)\n",
    "        self.best_reward = None\n",
    "        self.noise = 0.5\n",
    "        self.noise_max = 1.5\n",
    "        self.noise_min = 0.001\n",
    "        self.gamma = 0.98\n",
    "        \n",
    "    def get_action(self, state, policy=None):\n",
    "        if not policy:\n",
    "            actions = np.dot(state, self.policy)\n",
    "        else:\n",
    "            actions = np.dot(state, policy)\n",
    "        actions = np.exp(actions)\n",
    "        actions = actions / np.sum(actions)\n",
    "        return np.argmax(actions)\n",
    "            \n",
    "    def _noise_adder(self):\n",
    "        self.policy += np.random.normal(\n",
    "            loc=(self.noise_min + self.noise)/2,\n",
    "            scale=(self.noise - self.noise_min),\n",
    "            size=(self.n_observations, self.n_actions)\n",
    "        )\n",
    "    \n",
    "    def learn(self, discounted_reward):\n",
    "        if discounted_reward > self.best_reward:\n",
    "            if self.noise > self.noise_min:\n",
    "                self._noise_adder()\n",
    "        else:\n",
    "            if self.noise < self.noise_max:\n",
    "                self._noise_adder()\n",
    "        \n",
    "    def try_policy(self, env, evaluate=False):\n",
    "        discounted_reward = 0\n",
    "        state = env.reset()\n",
    "        np.reshape(state, (1,self.n_observations))\n",
    "        done = False\n",
    "        time_steps = 0\n",
    "        while not done:\n",
    "            time_steps += 1\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if not evaluate:\n",
    "                discounted_reward += (self.gamma ** time_steps) * reward\n",
    "            else:\n",
    "                discounted_reward += reward\n",
    "                env.render()\n",
    "        return discounted_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The environment handler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "EPOCHS = 300\n",
    "EVAL_FREQ = 5\n",
    "agent = HillClimber(env)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rewards_tracker = deque(maxlen=20)\n",
    "for epoch in range(EPOCHS):\n",
    "    discounted_reward = agent.try_policy(env)\n",
    "    agent.learn(discounted_reward)\n",
    "    if not epoch % EVAL_FREQ:\n",
    "        rewards_tracker.append(agent.try_policy(env, evaluate=True))\n",
    "        print(f\"Epoch : {epoch}    Evaluation Score : {rewards_tracker[-1]}    Noise Scale : {agent.noise}\")\n",
    "    if np.mean(rewards_tracker) > 195:\n",
    "        print(f\"Episode Solved in {epoch} epochs!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}