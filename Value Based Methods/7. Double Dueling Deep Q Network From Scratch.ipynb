{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is purpose of Double Dueling DQN?\n",
    "* Regular DQN tends to overestimate Q-values of potential actions in a given state\n",
    "* Once one specific action becomes overestimated, itâ€™s more likely to be chosen in the next iteration making it very \n",
    "hard for the agent to explore the environment uniformly and find the right policy.\n",
    "* We will use our primary network to select an action and a target network to generate a Q-value for that action.\n",
    "*  In order to synchronize our networks, we are going to copy weights from the primary network to the target one every \n",
    "'n' training steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the pong model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ENV_NAME = 'PongDeterministic-v4' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "We will have to downscale the image and grayscale it as it makes the policy faster. \n",
    "<br>\n",
    "You can use opencv or skimage or even inbuilt tensorflow methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ProcessFrame:\n",
    "    \"\"\"Convert to GrayScale and resize\"\"\"\n",
    "    def __init__(self, rows = 84, cols = 84):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            :param rows : Height to be resized to  \n",
    "            :param cols : Width to be resized to\n",
    "        \"\"\"\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Processes the frame passed to required using OpenCV\n",
    "        :param frame : An Atari game-play frame of (210,160,3)\n",
    "        :return:      A processed frame of (84,84,1)\n",
    "        \"\"\"\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        preprocessed_frame = cv2.resize(gray, [self.rows,self.cols])\n",
    "        return preprocessed_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the dueling network\n",
    "Refer to [Wang Et Al 2016](https://arxiv.org/abs/1511.06581) for more info. \n",
    "\n",
    "\n",
    "The architecture used :\n",
    "* The first convolutional layer has   32    8x8 filters with stride 4\n",
    "* The second convolutional layer has  64    4x4 filters with stride 2\n",
    "* The third convolutional layer has   64    3x3 filters with stride 1\n",
    "* The fourth convolutional layer has  1024  7x7 filters with stride 1\n",
    "\n",
    "Instead of directly predicting a single Q-value for each action, the dueling architecture \n",
    "splits the final convolutional layer into two streams that represent the value and advantage \n",
    "functions that predict a state value V(s) that depends only on the state, and action advantages \n",
    "A(s,a) that depend on the state and the respective action.\n",
    "\n",
    "An excerpt from the original paper :\n",
    "\n",
    "> Intuitively, the dueling architecture can learn which states are (or are not) valuable, without \n",
    "having to learn the effect of each action for each state. This is particularly useful in states \n",
    "where its actions do not affect the environment in any relevant way. In the experiments, we \n",
    "demonstrate that the dueling architecture can more quickly identify the correct action during \n",
    "policy evaluation as redundant or similar actions are added to the learning problem.\n",
    "\n",
    "The state value V(s) talks about is the state is favourable or not while the action advantage A(s,a)\n",
    "says which is the favourable action if in that state.  \n",
    "\n",
    "Now to combine the state and advantage values : \n",
    "\n",
    "> Next, we have to combine the value and advantage stream into $Q$-values $Q(s,a)$. This is done \n",
    "the following way :\n",
    "\\begin{equation}\n",
    "Q(s,a) = V(s) + \\left(A(s,a) - \\frac 1{| \\mathcal A |}\\sum_{a'}A(s, a')\\right)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "To give a sense of motion to the network, we stack 4 frames and pass to the network to train upon.\n",
    "\n",
    "Let's now implement this in a class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    \"\"\"Implementation of the brain\"\"\"\n",
    "    def __init__(self, n_actions, learning_rate = 0.00025, state_length = 4):\n",
    "        \"\"\"\n",
    "        Init all required variables for class\n",
    "        :param n_actions      : Number of actions available in the environment \n",
    "        :param learning_rate  : The specific lr argument for the Adam optimizer\n",
    "        :param state_length   : The number of frames which create a state\n",
    "        \"\"\"\n",
    "        self.n_actions = n_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.state_length = state_length\n",
    "        self.input_shape = [None, 84, 84, self.state_length]\n",
    "        self.input = tf.placeholder(shape=self.input_shape, dtype=tf.float32)\n",
    "        \n",
    "        self.input /= 255       # Normalisation to help the learning process  \n",
    "        \n",
    "        # Making the layers\n",
    "        self.conv1 = tf.layers.conv2d(inputs=self.input,\n",
    "                                      filters=32,\n",
    "                                      kernel_size=[8,8],\n",
    "                                      strides=4,\n",
    "                                      kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                      padding='valid',\n",
    "                                      activation=tf.nn.relu,\n",
    "                                      use_bias=False,\n",
    "                                      name='conv1')\n",
    "        self.conv2 = tf.layers.conv2d(inputs=self.conv1,\n",
    "                                      filters=64,\n",
    "                                      kernel_size=[4,4],\n",
    "                                      strides=2,\n",
    "                                      kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                      padding='valid',\n",
    "                                      activation=tf.nn.relu,\n",
    "                                      use_bias=False,\n",
    "                                      name='conv2')\n",
    "        self.conv3 = tf.layers.conv2d(inputs=self.conv2,\n",
    "                                      filters=64,\n",
    "                                      kernel_size=[3,3],\n",
    "                                      strides=1,\n",
    "                                      kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                      padding='valid',\n",
    "                                      activation=tf.nn.relu,\n",
    "                                      use_bias=False,\n",
    "                                      name='conv3')\n",
    "        self.conv4 = tf.layers.conv2d(inputs=self.conv3,\n",
    "                                      filters=1024,\n",
    "                                      kernel_size=[7,7],\n",
    "                                      strides=1,\n",
    "                                      kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                      padding='valid',\n",
    "                                      activation=tf.nn.relu,\n",
    "                                      use_bias=False,\n",
    "                                      name='conv4')\n",
    "        \n",
    "        # Making 2 Streams\n",
    "        self.valuestream, self.advantagestream = tf.split(value=self.conv4,\n",
    "                                                          num_or_size_splits=2,\n",
    "                                                          axis=3)\n",
    "        self.valuestream = tf.layers.flatten(self.valuestream)\n",
    "        self.advantagestream = tf.layers.flatten(self.advantagestream)\n",
    "        self.advantage = tf.layers.dense(inputs=self.advantagestream, \n",
    "                                         units=self.n_actions, \n",
    "                                         kernel_initializer=tf.variance_scaling_initializer(scale=2), \n",
    "                                         name=\"advantage\")\n",
    "        self.value = tf.layers.dense(inputs=self.valuestream, \n",
    "                                     units=1, \n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2), \n",
    "                                     name='value')\n",
    "        \n",
    "        # Combining value and advantage into Q-values\n",
    "        self.q_values = self.value + tf.subtract(self.advantage, \n",
    "                                                 tf.reduce_mean(self.advantage, \n",
    "                                                                axis=1, \n",
    "                                                                keepdims=True))\n",
    "        self.best_action = tf.argmax(self.q_values, axis=1)\n",
    "        \n",
    "        # targetQ according to Bellman equation: \n",
    "        # Q = r + gamma*max Q'\n",
    "        self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        # Action that was performed\n",
    "        self.action = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        # Q value of the action that was performed\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.q_values, \n",
    "                                           tf.one_hot(self.action, \n",
    "                                                      depth=self.n_actions, \n",
    "                                                      dtype=tf.float32)), \n",
    "                               axis=1)\n",
    "        \n",
    "        # Parameter updates\n",
    "        self.loss = tf.reduce_mean(tf.losses.huber_loss(labels=self.target_q, predictions=self.Q))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.update = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the Get Action Class\n",
    "\n",
    "For the first 50000 frames the agent only explores (Ïµ=1). Over the following 1 million frames, Ïµ is linearly\n",
    "decreased to 0.1\n",
    "\n",
    "After that we chose to decrease it to Ïµ=0.01 over the remaining frames as suggested by the OpenAi Baselines for DQN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GetAction:\n",
    "    \"\"\"To implement the Epsilon Greedy Policy for returning actions\"\"\"\n",
    "    def __init__(self, n_actions, eps_initial=1, eps_middle=0.1, eps_final=0.01, eps_eval=0,\n",
    "                 train_start=50e3, train_middle=1e6, train_max=25e6):\n",
    "        \"\"\"\n",
    "        Init all required variables for class\n",
    "        :param n_actions     : Action Space for the environment\n",
    "        :param eps_initial   : Start  Value for Epsilon\n",
    "        :param eps_middle    : Middle Value for Epsilon\n",
    "        :param eps_final     : Final  Value for Epsilon\n",
    "        :param eps_eval      : Epsilon Value to be used during Evaluation\n",
    "        :param train_start   : Number of frames to be pure exploration ie. Epsilon = 1\n",
    "        :param train_middle  : Number of frames for epsilon to decay from eps_initial to eps_middle \n",
    "        :param train_max     : Number of frames for epsilon to decay from eps_middle to eps_final\n",
    "        \"\"\"\n",
    "        self.n_actions = n_actions\n",
    "        self.eps_initial = eps_initial\n",
    "        self.eps_middle = eps_middle\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_eval = eps_eval\n",
    "        self.train_start = train_start\n",
    "        self.train_middle = train_middle\n",
    "        self.train_max = train_max\n",
    "        \n",
    "        # Now we need to generate linear lines for the decay between the start, middle, and the final points\n",
    "        # m = slope = y2-y1 / x2-x1\n",
    "        # c = intercept = y2 - m*x2\n",
    "        self.m_1 = (self.eps_middle - self.eps_initial) / (self.train_middle - self.train_start)\n",
    "        self.c_1 = self.eps_middle - self.m_1*self.train_middle\n",
    "        self.m_2 = (self.eps_final - self.eps_final) / (self.train_max - self.train_middle)\n",
    "        self.c_2 = self.eps_final - self.m_2*self.train_max\n",
    "    \n",
    "    def get_action(self, session, frame_number, state, dqn_object, evaluation=False):\n",
    "        \"\"\"\n",
    "        Returns the action based on the epsilon greedy policy for the given state\n",
    "        :param session       : The tensorflow session handling the prediction\n",
    "        :param frame_number  : Number of frames passed. Used to determine if to use (m_1,c_1) or (m_2,c_2)\n",
    "        :param state         : A stack of frames of the game-play after preprocessing ie. (84,84,4)\n",
    "        :param dqn_object   : DDQN object to return the best action\n",
    "        :param evaluation    : Flag to be set True while evaluation. Relies only upon the exploitation\n",
    "        :return: An integer between 0 and n_actions-1 to be set as the action\n",
    "        \"\"\"\n",
    "        if evaluation:\n",
    "            eps = self.eps_eval\n",
    "        elif frame_number < self.train_start:\n",
    "            eps = self.eps_initial\n",
    "        elif frame_number < self.train_middle:\n",
    "            eps = self.m_1*frame_number + self.c_1\n",
    "        elif frame_number < self.train_max:\n",
    "            eps = self.m_2*frame_number + self.c_2\n",
    "        \n",
    "        if np.random.random() < eps:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            return session.run(dqn_object.best_action, feed_dict={dqn_object.input:[state]})[0]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay!!\n",
    "\n",
    "> Second, learning directly from consecutive samples is inefficient, due to the strong correlations between \n",
    "the samples; randomizing the samples breaks these correlations and therefore reduces the variance of the updates. \n",
    "Third, when learning on-policy the current parameters determine the next data sample that the parameters are \n",
    "trained on. For example, if the maximizing action is to move left then the training samples will be dominated \n",
    "by samples from the left-hand side; if the maximizing action then switches to the right then the training \n",
    "distribution will also switch. It is easy to see how unwanted feedback loops may arise and the parameters \n",
    "could get stuck in a poor local minimum, or even diverge catastrophically. <br> [Mnih et al. 2013](https://arxiv.org/abs/1312.5602)\n",
    "\n",
    "We add the experiences to a memory holding *deque*. The experiences are of the form - $(state,action,reward,next\\_state,done)$.\n",
    "We store the last 1 million experiences. We fill up the deque with our experiences and sample a batch of random \n",
    "experiences when needed to train. The deque is repopulated as more and more experiences are seen and older ones are\n",
    "removed.\n",
    "\n",
    "Lets make the class. We will sample from a memory buffer of 1 Million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ExpMemory:\n",
    "    \"\"\"Storage for the experiences encountered . Stores the last 1 Million experiences\"\"\"\n",
    "    def __init__(self, size=1e6, rows=84, cols=84, state_length=4, batch=32):\n",
    "        \"\"\"\n",
    "        Init all required variables for class\n",
    "        :param size          : The number of experiences to be stored  \n",
    "        :param rows          : The pixel ht of a frame\n",
    "        :param cols          : The pixel wd of a frame\n",
    "        :param state_length  : The number of frames stacked to form a state\n",
    "        :param batch         : The size of a batch to train upon\n",
    "        \"\"\"\n",
    "        self.size = int(size)\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.state_length = state_length\n",
    "        self.batch = batch\n",
    "        self.count = 0      # Holds the number of experiences in the deque\n",
    "        self.current = 0    # Holds the index of the last updated experience\n",
    "        # Memory of experiences made:\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)                             # Placeholder for actions\n",
    "        self.rewards = np.empty(self.size, dtype=np.float32)                              # Placeholder for rewards                \n",
    "        self.dones = np.empty(self.size, dtype=np.bool)                                 # Placeholder for done\n",
    "        self.frames = np.empty((self.size,self.rows,self.cols), dtype=np.uint8)                         # Placeholder for frames\n",
    "        self.states = np.empty((self.batch,self.state_length,self.rows,self.cols), dtype=np.uint8)       # Placeholder for stacked frames\n",
    "        self.new_states = np.empty((self.batch,self.state_length,self.rows,self.cols), dtype=np.uint8)   # Placeholder for next stacked frames\n",
    "        self.indices = np.empty(self.batch, dtype=np.int32)                               # Placeholder for sample of experience tuples\n",
    "    def remember_state(self, action, frame, reward, done):\n",
    "        \"\"\"\n",
    "        Adds the tuple of experience to the memory buffer\n",
    "        :param action   : The action taken during the respective state \n",
    "        :param frame    : Preprocessed game-play frame (84,84,1)\n",
    "        :param reward   : Reward associated with action\n",
    "        :param done     : Done flag returned from taking the step\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if frame.shape != (self.rows,self.cols):\n",
    "            raise ValueError(f\"Wrong dimensions of frame. Reqd - {self.rows},{self.cols} \\t Passed - {frame.shape}\")\n",
    "        self.actions[self.current] = action             \n",
    "        self.frames[self.current, ...] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.dones[self.current] = done\n",
    "        self.current = (self.current+1) % self.size     # Implementation of DeQue on a list\n",
    "        self.count = max(self.current,self.count)      \n",
    "    def _get_state(self, index):\n",
    "        \"\"\"\n",
    "        Gets the corresponding stacked state for the associated index \n",
    "        :param index : The index query  \n",
    "        :return: A list of stacked frames of shape (4,84,84)\n",
    "        \"\"\"\n",
    "        if self.count>0 and index>self.state_length-1:\n",
    "            return self.frames[index-self.state_length+1 : index+1]\n",
    "        else:\n",
    "            raise ValueError(\"Minimum index of 3 is required. Populate the memory buffer\")\n",
    "    def _get_indices(self):\n",
    "        \"\"\"\n",
    "        Gets a list of indices which are valid for experience replay training\n",
    "        :return: A list of size (batch,)\n",
    "        \"\"\"\n",
    "        for i in range(self.batch):\n",
    "            while True:\n",
    "                index = np.random.randint(low=self.state_length, high=self.count)   # Select a random index\n",
    "                if index >= self.current >= index-self.state_length:                # State frames aren't continuous \n",
    "                    continue                                                        # ie. belong to two different times\n",
    "                if self.dones[index-self.state_length : index].any():               # If any of the frames before were part of a different life\n",
    "                    continue\n",
    "                break\n",
    "            self.indices[i] = index                                                 # Valid cases\n",
    "    def get_experience(self):\n",
    "        \"\"\"\n",
    "        Gets a batch of experience tuples of size self.batch\n",
    "        :return: states, actions, rewards, next_state, dones  with self.batch=32 replays\n",
    "        \"\"\"\n",
    "        if self.count<self.state_length:\n",
    "            raise ValueError(\"Too few experiences to get s batch\")\n",
    "        self._get_indices()\n",
    "        for i, index in enumerate(self.indices):\n",
    "            self.states[i] = self._get_state(index)\n",
    "            self.new_states[i] = self._get_state(index)\n",
    "        # We now use transpose just to get the states in the form of (32,84,84,4) -> (batch,rows,cols,state_length)\n",
    "        return np.transpose(self.states, axes=(0,2,3,1)), self.actions[self.indices], self.rewards[self.indices], \\\n",
    "               np.transpose(self.new_states, axes=(0,2,3,1)), self.rewards[self.indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# The Learning Part\n",
    "The problem is that both $Qprediction$ and $Qtarget$ depend on the same parameters $Î¸$ if only one network is used. This \n",
    "can lead to instability when regressing $Qprediction$ towards $Qtarget$ because the \"target is moving\". We ensure a \n",
    "\"fixed target\" by introducing a second network with fixed and only occasionally updated parameters that estimates the \n",
    "target Q-values.\n",
    "> Reinforcement learning is known to be unstable or even to diverge when a nonlinear function approximator such as a \n",
    "neural network is used to represent the action-value (also known as Q) function. This instability has several causes: \n",
    "the correlations present in the sequence of observations, the fact that small updates to Q may significantly change \n",
    "the policy and therefore change the data distribution, and the correlations between the action-values [...] and the \n",
    "target values [...]. We address these instabilities with a novel variant of Q-learning, which uses two key ideas. \n",
    "First, we used a biologically inspired mechanism termed experience replay that randomizes over the data, thereby \n",
    "removing correlations in the observation sequence and smoothing over changes in the data distribution [...]. \n",
    "Second, we used an iterative update that adjusts the action-values (Q) towards target values that are only \n",
    "periodically updated, thereby reducing correlations with the target.\n",
    "[Mnih et al. 2015](https://www.nature.com/articles/nature14236/)\n",
    "\n",
    "One network is used to predict the $Qprediction$ value while the other network is used to predict the $Qtarget$ value\n",
    "and is fixed. The main network is updated by gradient descent while every 10000 steps the prediction network value is \n",
    "copied to the target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def learn(sess, exp_memory, main_dqn, target_dqn, batch, gamma):\n",
    "    \"\"\"\n",
    "    Implements the DQN equation\n",
    "    :param sess         : A tensorflow session object\n",
    "    :param exp_memory   : An object of ExpMemory\n",
    "    :param main_dqn     : An object of DQN\n",
    "    :param target_dqn   : An object of DQN\n",
    "    :param batch        : A batch size to perform learning on\n",
    "    :param gamma        : Discounter\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    states, actions, rewards, next_states, dones = exp_memory.get_experience()\n",
    "\n",
    "    next_best_action = sess.run(main_dqn.best_action, feed_dict={main_dqn.input:next_states})   # Best action for next state from the main network\n",
    "    q_vals = sess.run(target_dqn.q_values, feed_dict={target_dqn.input:next_states})            # Q Values for next state with target network\n",
    "    double_q = q_vals[range(batch), next_best_action]\n",
    "\n",
    "    target_q = rewards + (gamma*double_q * (1-dones))           # Bellman Equation\n",
    "    # Gradient descend step to update the parameters of the main network\n",
    "    loss, _ = sess.run([main_dqn.loss, main_dqn.update], \n",
    "                          feed_dict={main_dqn.input:states, \n",
    "                                     main_dqn.target_q:target_q, \n",
    "                                     main_dqn.action:actions})\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy the networks\n",
    "After every specific number of steps, it is required to match the target q values to the main q values.\n",
    "\n",
    "We will now make a class for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class UpdateTargetNetwork:\n",
    "    \"\"\"Copies the weights of the main DQN to the target DQN\"\"\"\n",
    "    def __init__(self, main_dqn, target_dqn):\n",
    "        \"\"\"\n",
    "        Init all required variables for class\n",
    "        :param main_dqn     : An object of DQN \n",
    "        :param target_dqn   : An object of DQN\n",
    "        \"\"\"\n",
    "        self.main_dqn = main_dqn\n",
    "        self.target_dqn = target_dqn\n",
    "    def update_networks(self):\n",
    "        \"\"\"\n",
    "        Copies the weights of the model-1 to model-2\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.target_dqn.set_weights(self.main_dqn.get_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets just Explore the world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6) Box(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space, env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from time import sleep\n",
    "# env.reset()\n",
    "# for _ in range(1):\n",
    "#     done = False\n",
    "#     env.reset()\n",
    "#     while not done:\n",
    "#         obs, reward, done, info = env.step(env.action_space.sample())\n",
    "#         env.render()\n",
    "#         sleep(0.005)\n",
    "#     env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atari Player Class\n",
    "A class which controls the movements is helpful. Lets make one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Atari:\n",
    "    \"\"\"The player controller\"\"\"\n",
    "    def __init__(self, env_name, state_length=4):\n",
    "        \"\"\"\n",
    "        Init all required variables for class\n",
    "        :param env_name     : Which environment do u want?\n",
    "        :param state_length : The length of the stack of frames in a state\n",
    "        \"\"\"\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state_length = state_length\n",
    "        self.preprocess = ProcessFrame()\n",
    "        self.state = None\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Performs an action and observes the reward and done status of it\n",
    "        :param action   : Integer, the action to be done \n",
    "        :return: Experience tuple\n",
    "        \"\"\"\n",
    "        next_frame, reward, done, info = self.env.step(action)\n",
    "        processed_next_frame = self.preprocess.process_frame(next_frame)\n",
    "        new_state = np.append(self.state[:,:,1:], processed_next_frame, axis=2)     # Creating the states\n",
    "        self.state = new_state\n",
    "        \n",
    "        return processed_next_frame, reward, done, next_frame\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment and stacks four frames ontop of each other to create the first state\n",
    "        :return: done status\n",
    "        \"\"\"\n",
    "        frame = self.env.reset()\n",
    "        self.last_lives = 0\n",
    "\n",
    "        processed_frame = self.preprocess.process_frame(frame)\n",
    "        self.state = np.repeat(processed_frame, self.state_length, axis=2)\n",
    "        \n",
    "        return done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting all the Global variables\n",
    "This includes the constants and the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Control parameters\n",
    "import os\n",
    "import time\n",
    "MAX_EPISODE_LENGTH = 18000       # Equivalent of 5 minutes of gameplay at 60 frames per second\n",
    "EVAL_FREQUENCY = 200000          # Number of frames the agent sees between evaluations\n",
    "EVAL_STEPS = 10000               # Number of frames for one evaluation\n",
    "NETW_UPDATE_FREQ = 10000         # Number of chosen actions between updating the target network. \n",
    "                                 # According to Mnih et al. 2015 this is measured in the number of \n",
    "                                 # parameter updates (every four actions), however, in the \n",
    "                                 # DeepMind code, it is clearly measured in the number\n",
    "                                 # of actions the agent choses\n",
    "DISCOUNT_FACTOR = 0.99           # gamma in the Bellman equation\n",
    "REPLAY_MEMORY_START_SIZE = 50000 # Number of completely random actions, \n",
    "                                 # before the agent starts learning\n",
    "MAX_FRAMES = 30000000            # Total number of frames the agent sees \n",
    "MEMORY_SIZE = 1000000            # Number of transitions stored in the replay memory\n",
    "UPDATE_FREQ = 4                  # Every four actions a gradient descend step is performed\n",
    "LEARNING_RATE = 0.00001          # The learning rate to be used for the gradient descent in Adam\n",
    "BS = 32                          # Batch size\n",
    "\n",
    "PATH = \"output/\"                 # checkpoints will be saved here\n",
    "SUMMARIES = \"summaries\"          # logdir for tensorboard\n",
    "RUNID = str(time.time())\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "os.makedirs(os.path.join(SUMMARIES, RUNID), exist_ok=True)\n",
    "SUMM_WRITER = tf.summary.FileWriter(os.path.join(SUMMARIES, RUNID))\n",
    "\n",
    "atari = Atari(ENV_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the two DQN - $Target$ and $Main$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-4d6b0d6e7b05>:27: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\Space Invaders\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-4-4d6b0d6e7b05>:60: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-4d6b0d6e7b05>:65: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\Space Invaders\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:448: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\Space Invaders\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('mainDQN'):\n",
    "    MAIN_DQN = DQN(atari.env.action_space.n, LEARNING_RATE)  \n",
    "with tf.variable_scope('targetDQN'):\n",
    "    TARGET_DQN = DQN(atari.env.action_space.n)       \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TensorBoard\n",
    "We will have the loss and rewards in the visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Scalar summaries for tensorboard: loss, average reward and evaluation score\n",
    "with tf.name_scope('Performance'):\n",
    "    LOSS_PH = tf.placeholder(tf.float32, shape=None, name='loss_summary')\n",
    "    LOSS_SUMMARY = tf.summary.scalar('loss', LOSS_PH)\n",
    "    REWARD_PH = tf.placeholder(tf.float32, shape=None, name='reward_summary')\n",
    "    REWARD_SUMMARY = tf.summary.scalar('reward', REWARD_PH)\n",
    "    EVAL_SCORE_PH = tf.placeholder(tf.float32, shape=None, name='evaluation_summary')\n",
    "    EVAL_SCORE_SUMMARY = tf.summary.scalar('evaluation_score', EVAL_SCORE_PH)\n",
    "\n",
    "PERFORMANCE_SUMMARIES = tf.summary.merge([LOSS_SUMMARY, REWARD_SUMMARY])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Putting Everything Together\n",
    "Lets call all the classes and implement the game agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"Contains the training and evaluation loops\"\"\"\n",
    "    my_replay_memory = ExpMemory()\n",
    "    network_updater = UpdateTargetNetwork(MAIN_DQN, TARGET_DQN)\n",
    "    action_getter = GetAction(atari.env.action_space.n, train_max=MAX_FRAMES)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        frame_number = 0\n",
    "        rewards = []\n",
    "        loss_list = []\n",
    "        \n",
    "        while frame_number < MAX_FRAMES:\n",
    "            \n",
    "            ########################\n",
    "            ####### Training #######\n",
    "            ########################\n",
    "            epoch_frame = 0\n",
    "            while epoch_frame < EVAL_FREQUENCY:\n",
    "                done = atari.reset()\n",
    "                episode_reward_sum = 0\n",
    "                for _ in range(MAX_EPISODE_LENGTH):\n",
    "                    action = action_getter.get_action(sess, frame_number, atari.state, MAIN_DQN)   \n",
    "                    processed_new_frame, reward, done, _ = atari.step(action)  \n",
    "                    frame_number += 1\n",
    "                    epoch_frame += 1\n",
    "                    episode_reward_sum += reward\n",
    "                \n",
    "                    # Store transition in the replay memory\n",
    "                    my_replay_memory.remember_state(action=action, \n",
    "                                                    frame=processed_new_frame[:, :, 0],\n",
    "                                                    reward=reward, \n",
    "                                                    done=done)   \n",
    "                    \n",
    "                    if frame_number % UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        loss = learn(sess, my_replay_memory, MAIN_DQN, TARGET_DQN, BS, gamma = DISCOUNT_FACTOR)\n",
    "                        loss_list.append(loss)\n",
    "                        \n",
    "                    if frame_number % NETW_UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        network_updater.update_networks()\n",
    "                    \n",
    "                    if done:\n",
    "                        done = False\n",
    "                        break\n",
    "\n",
    "                rewards.append(episode_reward_sum)\n",
    "                \n",
    "                # Output the progress:\n",
    "                if len(rewards) % 10 == 0:\n",
    "                    # Scalar summaries for tensorboard\n",
    "                    if frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        summ = sess.run(PERFORMANCE_SUMMARIES, \n",
    "                                        feed_dict={LOSS_PH:np.mean(loss_list), \n",
    "                                                   REWARD_PH:np.mean(rewards[-100:])})\n",
    "                        \n",
    "                        SUMM_WRITER.add_summary(summ, frame_number)\n",
    "                        loss_list = []\n",
    "                        \n",
    "                    # Histogramm summaries for tensorboard\n",
    "                    print(len(rewards), frame_number, np.mean(rewards[-100:]))\n",
    "                    with open('rewards.dat', 'a') as reward_file:\n",
    "                        print(len(rewards), frame_number, \n",
    "                              np.mean(rewards[-100:]), file=reward_file)\n",
    "            \n",
    "            ########################\n",
    "            ###### Evaluation ######\n",
    "            ########################\n",
    "            terminal = True\n",
    "            eval_rewards = []\n",
    "            evaluate_frame_number = 0\n",
    "            \n",
    "            for _ in range(EVAL_STEPS):\n",
    "                if done:\n",
    "                    done = atari.reset(sess, evaluation=True)\n",
    "                    episode_reward_sum = 0\n",
    "                    done = False\n",
    "               \n",
    "                action_getter.get_action(sess, frame_number,\n",
    "                                         atari.state, \n",
    "                                         MAIN_DQN,\n",
    "                                         evaluation=True)\n",
    "                processed_new_frame, reward, done, new_frame = atari.step(action)\n",
    "                evaluate_frame_number += 1\n",
    "                episode_reward_sum += reward\n",
    "                \n",
    "                if done:\n",
    "                    eval_rewards.append(episode_reward_sum)\n",
    "                     \n",
    "            print(\"Evaluation score:\\n\", np.mean(eval_rewards))       \n",
    "            \n",
    "            #Save the network parameters\n",
    "            saver.save(sess, PATH+'/my_model', global_step=frame_number)\n",
    "            \n",
    "            # Show the evaluation score in tensorboard\n",
    "            summ = sess.run(EVAL_SCORE_SUMMARY, feed_dict={EVAL_SCORE_PH:np.mean(eval_rewards)})\n",
    "            SUMM_WRITER.add_summary(summ, frame_number)\n",
    "            with open('rewardsEval.dat', 'a') as eval_reward_file:\n",
    "                print(frame_number, np.mean(eval_rewards), file=eval_reward_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-f1dce124223e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\"Contains the training and evaluation loops\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmy_replay_memory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExpMemory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mnetwork_updater\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUpdateTargetNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAIN_DQN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTARGET_DQN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0maction_getter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGetAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0matari\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_FRAMES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-ec1226800d21>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, size, rows, cols, state_length, batch)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m                              \u001b[1;31m# Placeholder for rewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m                                 \u001b[1;31m# Placeholder for done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m                         \u001b[1;31m# Placeholder for frames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m       \u001b[1;31m# Placeholder for stacked frames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# Placeholder for next stacked frames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
