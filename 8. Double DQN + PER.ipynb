{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S3KcObXvXMUP",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_sv1SLz3g9OF"
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SO7XGSBNXMUS",
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exploring the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9AALONqIXMUT",
    "outputId": "763da7e8-a14c-494f-9d0a-161629090d61",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4482751,  0.       ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state = env.reset()\n",
    "np.array([state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7MzuPTYFXMUV",
    "outputId": "63582e85-277d-401d-de88-82bc8216182f",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3Oq914pgXMUY",
    "outputId": "ad17ca1b-f5aa-446a-be5c-31747ad900a0",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lgR7W9E6XMUa",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Human Solution\n",
    "Push in the direction of momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "L3tikuXGXMUb",
    "outputId": "106a8b08-cfec-494b-fa0d-df0c385f9bde",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs : [-0.5614274   0.00129274]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.55885155  0.00257584]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.55501181  0.00383975]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.54993681  0.005075  ]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.54366449  0.00627233]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.53624176  0.00742273]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.52772423  0.00851753]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.51817576  0.00954847]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.50766797  0.0105078 ]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.49627961  0.01138836]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.48409591  0.0121837 ]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.47120779  0.01288812]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.45771099  0.0134968 ]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.44370514  0.01400585]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.42929277  0.01441237]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.41457829  0.01471448]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.39966697  0.01491132]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.38466387  0.0150031 ]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.36967287  0.014991  ]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.35479573  0.01487715]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.34013118  0.01466455]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.32577421  0.01435697]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.31181535  0.01395886]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.29834014  0.01347521]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.28542869  0.01291145]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.27315535  0.01227333]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.26158855  0.01156681]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.25079062  0.01079792]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.24081788  0.00997275]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.2317206   0.00909728]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.22354319  0.00817741]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.21632436  0.00721883]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.21009729  0.00622707]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.20488986  0.00520743]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.20072484  0.00416502]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.19762009  0.00310475]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.1955887   0.00203139]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.19463915  0.00094955]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-1.94775368e-01 -1.36222398e-04]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.1979968  -0.00322143]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.20429002 -0.00629322]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.21362824 -0.00933822]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.22597037 -0.01234214]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.24125972 -0.01528935]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.25942234 -0.01816262]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.28036529 -0.02094295]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.30397485 -0.02360956]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.33011497 -0.02614013]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.3586261  -0.02851113]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.38932463 -0.03069853]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.4220032  -0.03267857]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.45643199 -0.03442879]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.49236114 -0.03592914]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.52952422 -0.03716308]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.56764286 -0.03811864]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.60643214 -0.03878927]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.64560653 -0.0391744 ]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.68488617 -0.03927963]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.7240028  -0.03911663]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.76270532 -0.03870253]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.80076443 -0.03805911]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.83797619 -0.03721176]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.87416444 -0.03618825]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.90918202 -0.03501759]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.94291086 -0.03372883]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.97526105 -0.03235019]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-1.00616923 -0.03090818]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-1.03559633 -0.0294271 ]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-1.06352494 -0.02792861]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-1.08995655 -0.02643161]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-1.1149087  -0.02495215]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-1.13841225 -0.02350355]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-1.16050882 -0.02209657]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-1.18124845 -0.02073963]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-1.2  0. ]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-1.1967581  0.0032419]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-1.19026366  0.00649445]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-1.18049596  0.00976769]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-1.16742551  0.01307045]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-1.15101593  0.01640959]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-1.13122662  0.01978931]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-1.10801632  0.02321029]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-1.08134752  0.0266688 ]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-1.05119184  0.03015569]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-1.01753633  0.03365551]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.98039071  0.03714562]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.93979513  0.04059558]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.89582821  0.04396692]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.84861466  0.04721355]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.79833178  0.05028288]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.7452139   0.05311789]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.68955367  0.05566023]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.63169956  0.05785411]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.57204884  0.05965072]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.51103601  0.06101282]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.44911739  0.06191862]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.38675274  0.06236465]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.32438585  0.06236689]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.26242569  0.06196016]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.20122997  0.06119572]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.14109236  0.0601376 ]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.08223413  0.05885823]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [-0.0248002   0.05743393]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [0.03114064 0.05594084]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [0.08559239 0.05445174]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [0.1386261  0.05303371]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [0.1903729  0.05174681]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [0.24101647 0.05064356]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [0.29078556 0.04976909]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [0.33994708 0.04916153]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [0.38879986 0.04885277]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [0.43766897 0.04886911]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [0.48690072 0.04923175]     Reward : -1.0     Done : False   Info : {}\n",
      "Obs : [0.5368578  0.04995707]     Reward : -1.0     Done : True   Info : {}\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "obs = env.reset()\n",
    "while not done:\n",
    "    action = 0 if obs[1]<0 else 2\n",
    "    obs,reward,done,info = env.step(action)\n",
    "    print(f\"Obs : {obs}     Reward : {reward}     Done : {done}   Info : {info}\")\n",
    "    sleep(.01)\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yr26oGjvXMUd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lets make the DQN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "at6gxfSaXMUd",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, env, learning_rate=0.001):\n",
    "        self.env = env\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.n_observations = env.observation_space.shape[0]\n",
    "        self.local_network = self._make_model()\n",
    "        self.target_network = self.local_network\n",
    "        self.experience_memory = deque(maxlen=3_00_000)\n",
    "        self.priorities = deque(maxlen=3_00_000)\n",
    "        self.batch_size = 64\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.alpha = 0.8\n",
    "        self.beta = 0.65\n",
    "        self.beta_max = 1.0\n",
    "        self.beta_growth = 1.005\n",
    "        self.offset = 0.1\n",
    "    def _make_model(self):\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(input_dim=self.n_observations,\n",
    "                                  units=32,\n",
    "                                  activation=tf.keras.activations.relu),\n",
    "            tf.keras.layers.Dense(units=16,\n",
    "                                  activation=tf.keras.activations.relu),\n",
    "            tf.keras.layers.Dense(units=self.n_actions,\n",
    "                                  activation=tf.keras.activations.linear)\n",
    "        ])\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(lr=0.0008),\n",
    "            loss=tf.keras.losses.mean_squared_error,\n",
    "            metrics=['accuracy']\n",
    "        )        \n",
    "        return model\n",
    "    def _append_memory(self, experience):\n",
    "        self.experience_memory.append(experience)\n",
    "        self.priorities.append(max(self.priorities, default=1))\n",
    "\n",
    "#         for i, ele in zip(range(len(self.priorities)), self.priorities):\n",
    "#             self.priorities[i] = ele ** self.alpha\n",
    "#         priorities_sum = sum(self.priorities)\n",
    "#         for i, ele in zip(range(len(self.priorities)), self.priorities):\n",
    "#             self.priorities[i] = ele / priorities_sum\n",
    "        return True if len(self.experience_memory) > self.batch_size else False\n",
    "    def get_action(self, state, eval=False):\n",
    "        if eval:\n",
    "            return np.argmax(self.local_network.predict(state)[0])\n",
    "        else:\n",
    "            greedy_action = np.argmax(self.local_network.predict(state)[0])\n",
    "            random_action = self.env.action_space.sample()\n",
    "            return random_action if np.random.random() < self.epsilon else greedy_action\n",
    "    def _get_propability(self):\n",
    "        propabilities = np.array(self.priorities)\n",
    "        propabilities = propabilities ** self.alpha\n",
    "        propabilities_sum = sum(propabilities)\n",
    "        propabilities /= propabilities_sum\n",
    "        return list(propabilities)\n",
    "\n",
    "    def train(self, experience):\n",
    "        if self._append_memory(experience):\n",
    "            propabilities = self._get_propability()\n",
    "            indices = np.random.choice(a=len(self.experience_memory),\n",
    "                                       size=self.batch_size,\n",
    "                                       p=propabilities)\n",
    "            states = []\n",
    "            local_q_values = []\n",
    "            imp_sample_weights = []\n",
    "            for i in indices:\n",
    "                state, action, reward, next_state, done = self.experience_memory[i]\n",
    "                states.append(state.tolist())\n",
    "                imp_sample_weights.append((len(self.experience_memory)*propabilities[i])**(-self.beta))\n",
    "                if done:\n",
    "                    q_target = reward\n",
    "                else:\n",
    "                    q_target = reward + self.discount * \\\n",
    "                               self.target_network.predict(next_state)[0][np.argmax(self.local_network.predict(next_state)[0])]\n",
    "                p = abs(q_target - self.local_network.predict(state)[0][action]) + self.offset\n",
    "                self.priorities[i] = p\n",
    "                local_q_values.append(self.local_network.predict(state).tolist())\n",
    "                local_q_values[-1][0][action] = q_target\n",
    "            self.local_network.fit(\n",
    "                np.squeeze(states), np.squeeze(local_q_values), \n",
    "                sample_weight=np.array(imp_sample_weights),\n",
    "                batch_size=self.batch_size,\n",
    "                verbose=0\n",
    "            )\n",
    "#             self.epsilon = max(self.epsilon_min, self.epsilon*self.epsilon_decay)\n",
    "#             self.beta = min(self.beta_max, self.beta*self.beta_growth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "1LtXQysuXMUf",
    "outputId": "1b61c8eb-6bc7-4adf-85ac-4a4c13752f05",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = DQN(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M_gzlun8XMUh",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_agent():\n",
    "    eval_reward = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    state = np.array([state])\n",
    "    while not done:\n",
    "        action = np.array(agent.get_action(state, eval=True))\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.array([next_state])\n",
    "        env.render()\n",
    "        sleep(0.01)\n",
    "        state = next_state\n",
    "        eval_reward += reward\n",
    "    env.close()\n",
    "    return eval_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-200.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1404
    },
    "colab_type": "code",
    "id": "12yhnDXPXMUj",
    "outputId": "4d294f03-6feb-4b82-ea06-b321efdf4292",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 0    Reward : -200.0    Epsilon : 0.9801    Beta : 0.6565162499999999\n",
      "\n",
      "Target Network Weights Updated \n",
      "Evaluation Score : -200.0\n",
      "Episode : 1    Reward : -200.0    Epsilon : 0.96059601    Beta : 0.6630978254062497\n",
      "Episode : 2    Reward : -200.0    Epsilon : 0.9414801494009999    Beta : 0.6697453811059473\n",
      "Episode : 3    Reward : -200.0    Epsilon : 0.92274469442792    Beta : 0.6764595785515343\n",
      "Episode : 4    Reward : -200.0    Epsilon : 0.9043820750088043    Beta : 0.6832410858265132\n"
     ]
    }
   ],
   "source": [
    "for epi in range(30):\n",
    "    reward_epi = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    state = np.array([state])\n",
    "    t=0\n",
    "    while not done:\n",
    "        t+=1\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        next_state = np.array([next_state])\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        if (t%5)==0:\n",
    "            agent.train(experience)\n",
    "        else:\n",
    "            agent._append_memory(experience)\n",
    "        if (t%100)==0:\n",
    "            agent.epsilon = max(agent.epsilon_min, agent.epsilon*agent.epsilon_decay)\n",
    "            agent.beta = min(agent.beta_max, agent.beta*agent.beta_growth)\n",
    "        state = next_state\n",
    "        reward_epi += reward\n",
    "    print(f\"Episode : {epi}    Reward : {reward_epi}    Epsilon : {agent.epsilon}    Beta : {agent.beta}\")\n",
    "    if epi%5 == 0:\n",
    "        agent.target_network.set_weights(agent.local_network.get_weights())\n",
    "        print(f\"\\nTarget Network Weights Updated \\nEvaluation Score : {evaluate_agent()}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Double DQN + PER.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
