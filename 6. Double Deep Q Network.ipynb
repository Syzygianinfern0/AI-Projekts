{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space : Discrete(3) \n",
      " Observation Space : Box(2,)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "print(f\"Action Space : {env.action_space} \\n Observation Space : {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_6 (Flatten)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 24)                72        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 747\n",
      "Trainable params: 747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(1, env.observation_space.shape[0])),\n",
    "    keras.layers.Dense(24, activation=keras.activations.relu),\n",
    "    keras.layers.Dense(24, activation=keras.activations.relu),\n",
    "    keras.layers.Dense(env.action_space.n, activation=keras.activations.linear)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "  9/200 [>.............................] - ETA: 2:56 - reward: 1.0000 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Space Invaders\\lib\\site-packages\\keras_rl-0.4.2-py3.7.egg\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/200 [=============>................] - ETA: 12s - reward: 1.0000done, took 11.454 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x139a06c4550>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = DQNAgent(model=model,\n",
    "               policy=EpsGreedyQPolicy(),\n",
    "               memory=SequentialMemory(limit=50000, window_length=1),\n",
    "               nb_actions=env.action_space.n,\n",
    "               nb_steps_warmup=10)\n",
    "dqn.compile(optimizer=keras.optimizers.Adam(), \n",
    "            metrics=[keras.metrics.mean_absolute_error])\n",
    "dqn.fit(env=env,\n",
    "        nb_steps=100,\n",
    "        visualize=True,\n",
    "        nb_max_episode_steps=200,\n",
    "        verbose=1,\n",
    "        log_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "  7/200 [>.............................] - ETA: 29s - reward: 1.0000 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Space Invaders\\lib\\site-packages\\keras_rl-0.4.2-py3.7.egg\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 5s 25ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 161.000 [161.000, 161.000] - loss: 0.079 - mean_absolute_error: 29.482 - mean_q: 59.608\n",
      "\n",
      "Interval 2 (200 steps performed)\n",
      "200/200 [==============================] - 2s 11ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 191.000 [191.000, 191.000] - loss: 8.379 - mean_absolute_error: 32.610 - mean_q: 65.570\n",
      "\n",
      "Interval 3 (400 steps performed)\n",
      "200/200 [==============================] - 2s 11ms/step - reward: 1.0000\n",
      "2 episodes - episode_reward: 103.500 [61.000, 146.000] - loss: 64.346 - mean_absolute_error: 36.499 - mean_q: 73.228\n",
      "\n",
      "Interval 4 (600 steps performed)\n",
      "200/200 [==============================] - 2s 11ms/step - reward: 1.0000\n",
      "2 episodes - episode_reward: 99.000 [77.000, 121.000] - loss: 20.047 - mean_absolute_error: 34.703 - mean_q: 70.327\n",
      "\n",
      "Interval 5 (800 steps performed)\n",
      "200/200 [==============================] - 2s 11ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 123.000 [123.000, 123.000] - loss: 37.608 - mean_absolute_error: 33.799 - mean_q: 68.272\n",
      "\n",
      "Interval 6 (1000 steps performed)\n",
      "200/200 [==============================] - 2s 12ms/step - reward: 1.0000\n",
      "2 episodes - episode_reward: 152.500 [134.000, 171.000] - loss: 19.214 - mean_absolute_error: 32.692 - mean_q: 66.048\n",
      "\n",
      "Interval 7 (1200 steps performed)\n",
      "200/200 [==============================] - 2s 11ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 156.000 [156.000, 156.000] - loss: 19.464 - mean_absolute_error: 32.120 - mean_q: 65.097\n",
      "\n",
      "Interval 8 (1400 steps performed)\n",
      "200/200 [==============================] - 2s 11ms/step - reward: 1.0000\n",
      "Interval 9 (1600 steps performed)\n",
      "200/200 [==============================] - 2s 11ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 367.000 [367.000, 367.000] - loss: 7.545 - mean_absolute_error: 31.909 - mean_q: 64.919\n",
      "\n",
      "Interval 10 (1800 steps performed)\n",
      "200/200 [==============================] - 2s 11ms/step - reward: 1.0000\n",
      "Interval 11 (2000 steps performed)\n",
      "200/200 [==============================] - 2s 11ms/step - reward: 1.0000\n",
      "Interval 12 (2200 steps performed)\n",
      "200/200 [==============================] - 2s 12ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 500.000 [500.000, 500.000] - loss: 28.066 - mean_absolute_error: 37.064 - mean_q: 74.927\n",
      "\n",
      "Interval 13 (2400 steps performed)\n",
      "200/200 [==============================] - 2s 11ms/step - reward: 1.0000\n",
      "Interval 14 (2600 steps performed)\n",
      "200/200 [==============================] - 2s 11ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 500.000 [500.000, 500.000] - loss: 16.069 - mean_absolute_error: 38.753 - mean_q: 78.415\n",
      "\n",
      "Interval 15 (2800 steps performed)\n",
      "200/200 [==============================] - 2s 11ms/step - reward: 1.0000\n",
      "Interval 16 (3000 steps performed)\n",
      "200/200 [==============================] - 2s 11ms/step - reward: 1.0000\n",
      "Interval 17 (3200 steps performed)\n",
      "200/200 [==============================] - 2s 12ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 500.000 [500.000, 500.000] - loss: 6.367 - mean_absolute_error: 42.707 - mean_q: 86.358\n",
      "\n",
      "Interval 18 (3400 steps performed)\n",
      "200/200 [==============================] - 3s 13ms/step - reward: 1.0000\n",
      "Interval 19 (3600 steps performed)\n",
      "200/200 [==============================] - 2s 12ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 500.000 [500.000, 500.000] - loss: 27.498 - mean_absolute_error: 46.416 - mean_q: 93.405\n",
      "\n",
      "Interval 20 (3800 steps performed)\n",
      "200/200 [==============================] - 2s 12ms/step - reward: 1.0000\n",
      "Interval 21 (4000 steps performed)\n",
      "200/200 [==============================] - 3s 13ms/step - reward: 1.0000\n",
      "Interval 22 (4200 steps performed)\n",
      "200/200 [==============================] - 2s 11ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 500.000 [500.000, 500.000] - loss: 12.911 - mean_absolute_error: 48.953 - mean_q: 98.387\n",
      "\n",
      "Interval 23 (4400 steps performed)\n",
      "200/200 [==============================] - 2s 11ms/step - reward: 1.0000\n",
      "Interval 24 (4600 steps performed)\n",
      "200/200 [==============================] - 2s 11ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 500.000 [500.000, 500.000] - loss: 23.315 - mean_absolute_error: 50.487 - mean_q: 101.248\n",
      "\n",
      "Interval 25 (4800 steps performed)\n",
      "200/200 [==============================] - 2s 11ms/step - reward: 1.0000\n",
      "done, took 59.906 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x139b758b828>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_actions = env.action_space.n\n",
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(keras.optimizers.Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=5000, visualize=True, verbose=1, log_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 500.000, steps: 500\n",
      "Episode 2: reward: 500.000, steps: 500\n",
      "Episode 3: reward: 500.000, steps: 500\n",
      "Episode 4: reward: 500.000, steps: 500\n",
      "Episode 5: reward: 500.000, steps: 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x139a2692b38>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights(filepath='weights/6_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = DQNAgent(model=model,\n",
    "               policy=EpsGreedyQPolicy(),\n",
    "               memory=SequentialMemory(limit=50000, window_length=1),\n",
    "               nb_actions=env.action_space.n,\n",
    "               nb_steps_warmup=10)\n",
    "test.compile(optimizer=keras.optimizers.Adam(), \n",
    "            metrics=[keras.metrics.mean_absolute_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.load_weights(filepath='weights/6_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.test(en)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
