{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# What is purpose of Double Dueling DQN?\n",
    "* Regular DQN tends to overestimate Q-values of potential actions in a given state\n",
    "* Once one specific action becomes overestimated, itâ€™s more likely to be chosen in the next iteration making it very \n",
    "hard for the agent to explore the environment uniformly and find the right policy.\n",
    "* We will use our primary network to select an action and a target network to generate a Q-value for that action.\n",
    "*  In order to synchronize our networks, we are going to copy weights from the primary network to the target one every \n",
    "'n' training steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will be using the pong model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ENV_NAME = 'PongDeterministic-v4' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing\n",
    "We will have to downscale the image and grayscale it as it makes the policy faster. \n",
    "<br>\n",
    "You can use opencv or skimage or even inbuilt tensorflow methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ProcessFrame:\n",
    "    \"\"\"Convert to GrayScale and resize\"\"\"\n",
    "    def __init__(self, rows = 84, cols = 84):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            :param rows : Height to be resized to  \n",
    "            :param cols : Width to be resized to\n",
    "        \"\"\"\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Processes the frame passed to required using OpenCV\n",
    "        :param frame : An Atari game-play frame of (210,160,3)\n",
    "        :return:      A processed frame of (84,84,1)\n",
    "        \"\"\"\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        resized = cv2.resize(gray, [self.rows,self.cols])\n",
    "        preprocessed_frame = resized / 255.0\n",
    "        return preprocessed_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Making the dueling network\n",
    "Refer to [Wang Et Al 2016](https://arxiv.org/abs/1511.06581) for more info. \n",
    "\n",
    "\n",
    "The architecture used :\n",
    "* The first convolutional layer has   32    8x8 filters with stride 4\n",
    "* The second convolutional layer has  64    4x4 filters with stride 2\n",
    "* The third convolutional layer has   64    3x3 filters with stride 1\n",
    "* The fourth convolutional layer has  1024  7x7 filters with stride 1\n",
    "\n",
    "Instead of directly predicting a single Q-value for each action, the dueling architecture \n",
    "splits the final convolutional layer into two streams that represent the value and advantage \n",
    "functions that predict a state value V(s) that depends only on the state, and action advantages \n",
    "A(s,a) that depend on the state and the respective action.\n",
    "\n",
    "An excerpt from the original paper :\n",
    "\n",
    "> Intuitively, the dueling architecture can learn which states are (or are not) valuable, without \n",
    "having to learn the effect of each action for each state. This is particularly useful in states \n",
    "where its actions do not affect the environment in any relevant way. In the experiments, we \n",
    "demonstrate that the dueling architecture can more quickly identify the correct action during \n",
    "policy evaluation as redundant or similar actions are added to the learning problem.\n",
    "\n",
    "The state value V(s) talks about is the state is favourable or not while the action advantage A(s,a)\n",
    "says which is the favourable action if in that state.  \n",
    "\n",
    "Now to combine the state and advantage values : \n",
    "\n",
    "> Next, we have to combine the value and advantage stream into $Q$-values $Q(s,a)$. This is done \n",
    "the following way :\n",
    "\\begin{equation}\n",
    "Q(s,a) = V(s) + \\left(A(s,a) - \\frac 1{| \\mathcal A |}\\sum_{a'}A(s, a')\\right)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "To give a sense of motion to the network, we stack 4 frames and pass to the network to train upon.\n",
    "\n",
    "Let's now implement this in a class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 1, 1, 1024)        3212288   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 3,291,297\n",
      "Trainable params: 3,291,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 1, 1, 1024)        3212288   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 3,294,372\n",
      "Trainable params: 3,294,372\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class DQN:\n",
    "    \"\"\"Implementation of the brain\"\"\"\n",
    "    def __init__(self, n_actions, stream,learning_rate = 0.00025, state_length = 4):\n",
    "        \"\"\"\n",
    "        Init all required variables for class\n",
    "        :param n_actions      : Number of actions available in the environment \n",
    "        :param stream         : Type of stream - 'v' or 'a' for Advantage and Value respectively\n",
    "        :param learning_rate  : The specific lr argument for the Adam optimizer\n",
    "        :param state_length   : The number of frames which create a state\n",
    "        \"\"\"\n",
    "        self.n_actions = n_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.agent_history_length = state_length\n",
    "        self.stream = stream\n",
    "        self.input_shape = [84, 84, self.agent_history_length]\n",
    "        self.make_model()\n",
    "        \n",
    "    def make_model(self):\n",
    "        \"\"\"\n",
    "        Creates the neural network responsible\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=[8,8], strides=4,\n",
    "                                   input_shape=self.input_shape, \n",
    "                                   activation='relu',\n",
    "                                   kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2)),\n",
    "            tf.keras.layers.Conv2D(filters=64, kernel_size=[4,4], strides=2,\n",
    "                                   input_shape=self.input_shape, \n",
    "                                   activation='relu',\n",
    "                                   kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2)),\n",
    "            tf.keras.layers.Conv2D(filters=64, kernel_size=[3,3], strides=1,\n",
    "                                   input_shape=self.input_shape, \n",
    "                                   activation='relu',\n",
    "                                   kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2)),\n",
    "            tf.keras.layers.Conv2D(filters=1024, kernel_size=[7,7], strides=1,\n",
    "                                   input_shape=self.input_shape, \n",
    "                                   activation='relu',\n",
    "                                   kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2)),\n",
    "            tf.keras.layers.Flatten()\n",
    "        ])\n",
    "        if self.stream == 'v':\n",
    "            self.model.add(\n",
    "                tf.keras.layers.Dense(units=1,\n",
    "                                      kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        elif self.stream == 'a':\n",
    "            self.model.add(\n",
    "                tf.keras.layers.Dense(units=self.n_actions,\n",
    "                                      kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "            \n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate),\n",
    "                           loss=tf.keras.losses.mean_squared_error,\n",
    "                           metrics=['accuracy'])\n",
    "        self.model.summary()\n",
    "Value = DQN(4, 'v')\n",
    "Advantage = DQN(4, 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Making the Get Action Class\n",
    "\n",
    "For the first 50000 frames the agent only explores (Ïµ=1). Over the following 1 million frames, Ïµ is linearly\n",
    "decreased to 0.1\n",
    "\n",
    "After that we chose to decrease it to Ïµ=0.01 over the remaining frames as suggested by the OpenAi Baselines for DQN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GetAction:\n",
    "    \"\"\"To implement the Epsilon Greedy Policy for returning actions\"\"\"\n",
    "    def __init__(self, n_actions, eps_initial=1, eps_middle=0.1, eps_final=0.01, eps_eval=0,\n",
    "                 train_start=50e3, train_middle=1e6, train_max=25e6):\n",
    "        \"\"\"\n",
    "        Init all required variables for class\n",
    "        :param n_actions     : Action Space for the environment\n",
    "        :param eps_initial   : Start  Value for Epsilon\n",
    "        :param eps_middle    : Middle Value for Epsilon\n",
    "        :param eps_final     : Final  Value for Epsilon\n",
    "        :param eps_eval      : Epsilon Value to be used during Evaluation\n",
    "        :param train_start   : Number of frames to be pure exploration ie. Epsilon = 1\n",
    "        :param train_middle  : Number of frames for epsilon to decay from eps_initial to eps_middle \n",
    "        :param train_max     : Number of frames for epsilon to decay from eps_middle to eps_final\n",
    "        \"\"\"\n",
    "        self.n_actions = n_actions\n",
    "        self.eps_initial = eps_initial\n",
    "        self.eps_middle = eps_middle\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_eval = eps_eval\n",
    "        self.train_start = train_start\n",
    "        self.train_middle = train_middle\n",
    "        self.train_max = train_max\n",
    "        \n",
    "        # Now we need to generate linear lines for the decay between the start, middle, and the final points\n",
    "        # m = slope = y2-y1 / x2-x1\n",
    "        # c = intercept = y2 - m*x2\n",
    "        self.m_1 = (self.eps_middle - self.eps_initial) / (self.train_middle - self.train_start)\n",
    "        self.c_1 = self.eps_middle - self.m_1*self.train_middle\n",
    "        self.m_2 = (self.eps_final - self.eps_final) / (self.train_max - self.train_middle)\n",
    "        self.c_2 = self.eps_final - self.m_2*self.train_max\n",
    "    \n",
    "    def get_action(self, frame_number, state, dqn_object, evaluation=False):\n",
    "        \"\"\"\n",
    "        Returns the action based on the epsilon greedy policy for the given state\n",
    "        :param frame_number  : Number of frames passed. Used to determine if to use (m_1,c_1) or (m_2,c_2)\n",
    "        :param state         : A stack of frames of the game-play after preprocessing ie. (84,84,4)\n",
    "        :param dqn_object    : DQN object to return the best action\n",
    "        :param evaluation    : Flag to be set True while evaluation. Relies only upon the exploitation\n",
    "        :return: An integer between 0 and n_actions-1 to be set as the action\n",
    "        \"\"\"\n",
    "        if evaluation:\n",
    "            eps = self.eps_eval\n",
    "        elif frame_number < self.train_start:\n",
    "            eps = self.eps_initial\n",
    "        elif frame_number < self.train_middle:\n",
    "            eps = self.m_1*frame_number + self.c_1\n",
    "        elif frame_number < self.train_max:\n",
    "            eps = self.m_2*frame_number + self.c_2\n",
    "        \n",
    "        if np.random.random() < eps:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            return dqn_object.predcit(state)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Experience Replay!!\n",
    "\n",
    "> Second, learning directly from consecutive samples is inefficient, due to the strong correlations between \n",
    "the samples; randomizing the samples breaks these correlations and therefore reduces the variance of the updates. \n",
    "Third, when learning on-policy the current parameters determine the next data sample that the parameters are \n",
    "trained on. For example, if the maximizing action is to move left then the training samples will be dominated \n",
    "by samples from the left-hand side; if the maximizing action then switches to the right then the training \n",
    "distribution will also switch. It is easy to see how unwanted feedback loops may arise and the parameters \n",
    "could get stuck in a poor local minimum, or even diverge catastrophically. <br> [Mnih et al. 2013](https://arxiv.org/abs/1312.5602)\n",
    "\n",
    "We add the experiences to a memory holding *deque*. The experiences are of the form - $(state,action,reward,next\\_state,done)$.\n",
    "We store the last 1 million experiences. We fill up the deque with our experiences and sample a batch of random \n",
    "experiences when needed to train. The deque is repopulated as more and more experiences are seen and older ones are\n",
    "removed.\n",
    "\n",
    "Lets make the class. We will sample from a memory buffer of 1 Million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class ReplayMemory:\n",
    "    \"\"\"Replay Memory that stores the last size=1,000,000 transitions\"\"\"\n",
    "    def __init__(self, size=1000000, frame_height=84, frame_width=84, \n",
    "                 agent_history_length=4, batch_size=32):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size: Integer, Number of stored transitions\n",
    "            frame_height: Integer, Height of a frame of an Atari game\n",
    "            frame_width: Integer, Width of a frame of an Atari game\n",
    "            agent_history_length: Integer, Number of frames stacked together to create a state\n",
    "            batch_size: Integer, Number if transitions returned in a minibatch\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.agent_history_length = agent_history_length\n",
    "        self.batch_size = batch_size\n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "        \n",
    "        # Pre-allocate memory\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
    "        self.frames = np.empty((self.size, self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.terminal_flags = np.empty(self.size, dtype=np.bool)\n",
    "        \n",
    "        # Pre-allocate memory for the states and new_states in a minibatch\n",
    "        self.states = np.empty((self.batch_size, self.agent_history_length, \n",
    "                                self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.new_states = np.empty((self.batch_size, self.agent_history_length, \n",
    "                                    self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.indices = np.empty(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "    def add_experience(self, action, frame, reward, terminal):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action: An integer between 0 and env.action_space.n - 1 \n",
    "                determining the action the agent perfomed\n",
    "            frame: A (84, 84, 1) frame of an Atari game in grayscale\n",
    "            reward: A float determining the reward the agend received for performing an action\n",
    "            terminal: A bool stating whether the episode terminated\n",
    "        \"\"\"\n",
    "        if frame.shape != (self.frame_height, self.frame_width):\n",
    "            raise ValueError('Dimension of frame is wrong!')\n",
    "        self.actions[self.current] = action\n",
    "        self.frames[self.current, ...] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.terminal_flags[self.current] = terminal\n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.size\n",
    "             \n",
    "    def _get_state(self, index):\n",
    "        if self.count is 0:\n",
    "            raise ValueError(\"The replay memory is empty!\")\n",
    "        if index < self.agent_history_length - 1:\n",
    "            raise ValueError(\"Index must be min 3\")\n",
    "        return self.frames[index-self.agent_history_length+1:index+1, ...]\n",
    "        \n",
    "    def _get_valid_indices(self):\n",
    "        for i in range(self.batch_size):\n",
    "            while True:\n",
    "                index = random.randint(self.agent_history_length, self.count - 1)\n",
    "                if index < self.agent_history_length:\n",
    "                    continue\n",
    "                if index >= self.current and index - self.agent_history_length <= self.current:\n",
    "                    continue\n",
    "                if self.terminal_flags[index - self.agent_history_length:index].any():\n",
    "                    continue\n",
    "                break\n",
    "            self.indices[i] = index\n",
    "            \n",
    "    def get_minibatch(self):\n",
    "        \"\"\"\n",
    "        Returns a minibatch of self.batch_size = 32 transitions\n",
    "        \"\"\"\n",
    "        if self.count < self.agent_history_length:\n",
    "            raise ValueError('Not enough memories to get a minibatch')\n",
    "        \n",
    "        self._get_valid_indices()\n",
    "            \n",
    "        for i, idx in enumerate(self.indices):\n",
    "            self.states[i] = self._get_state(idx - 1)\n",
    "            self.new_states[i] = self._get_state(idx)\n",
    "        \n",
    "        return np.transpose(self.states, axes=(0, 2, 3, 1)), self.actions[self.indices], self.rewards[self.indices], np.transpose(self.new_states, axes=(0, 2, 3, 1)), self.terminal_flags[self.indices]\n",
    "                "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}