{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# What is purpose of Double Dueling DQN?\n",
    "* Regular DQN tends to overestimate Q-values of potential actions in a given state\n",
    "* Once one specific action becomes overestimated, itâ€™s more likely to be chosen in the next iteration making it very \n",
    "hard for the agent to explore the environment uniformly and find the right policy.\n",
    "* We will use our primary network to select an action and a target network to generate a Q-value for that action.\n",
    "*  In order to synchronize our networks, we are going to copy weights from the primary network to the target one every \n",
    "'n' training steps.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will be using the pong model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "ENV_NAME = 'PongDeterministic-v4' "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing\n",
    "We will have to downscale the image and grayscale it as it makes the policy faster. \n",
    "<br>\n",
    "You can use opencv or skimage or even inbuilt tensorflow methods\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class ProcessFrame:\n",
    "    \"\"\"Convert to GrayScale and resize\"\"\"\n",
    "    def __init__(self, rows = 84, cols = 84):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            :param rows : Height to be resized to  \n",
    "            :param cols : Width to be resized to\n",
    "        \"\"\"\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Processes the frame passed to required using OpenCV\n",
    "        :param frame : An Atari game-play frame of (210,160,3)\n",
    "        :return:      A processed frame of (84,84,1)\n",
    "        \"\"\"\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        resized = cv2.resize(gray, [self.rows,self.cols])\n",
    "        preprocessed_frame = resized / 255.0\n",
    "        return preprocessed_frame"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Making the dueling network\n",
    "Refer to [Wang Et Al 2016](https://arxiv.org/abs/1511.06581) for more info. \n",
    "\n",
    "\n",
    "The architecture used :\n",
    "* The first convolutional layer has   32    8x8 filters with stride 4\n",
    "* The second convolutional layer has  64    4x4 filters with stride 2\n",
    "* The third convolutional layer has   64    3x3 filters with stride 1\n",
    "* The fourth convolutional layer has  1024  7x7 filters with stride 1\n",
    "\n",
    "Instead of directly predicting a single Q-value for each action, the dueling architecture \n",
    "splits the final convolutional layer into two streams that represent the value and advantage \n",
    "functions that predict a state value V(s) that depends only on the state, and action advantages \n",
    "A(s,a) that depend on the state and the respective action.\n",
    "\n",
    "An excerpt from the original paper :\n",
    "\n",
    "> Intuitively, the dueling architecture can learn which states are (or are not) valuable, without \n",
    "having to learn the effect of each action for each state. This is particularly useful in states \n",
    "where its actions do not affect the environment in any relevant way. In the experiments, we \n",
    "demonstrate that the dueling architecture can more quickly identify the correct action during \n",
    "policy evaluation as redundant or similar actions are added to the learning problem.\n",
    "\n",
    "The state value V(s) talks about is the state is favourable or not while the action advantage A(s,a)\n",
    "says which is the favourable action if in that state.  \n",
    "\n",
    "Now to combine the state and advantage values : \n",
    "\n",
    "> Next, we have to combine the value and advantage stream into $Q$-values $Q(s,a)$. This is done \n",
    "the following way :\n",
    "\\begin{equation}\n",
    "Q(s,a) = V(s) + \\left(A(s,a) - \\frac 1{| \\mathcal A |}\\sum_{a'}A(s, a')\\right)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "To give a sense of motion to the network, we stack 4 frames and pass to the network to train upon.\n",
    "\n",
    "Let's now implement this in a class. \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\Space Invaders\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n",
      "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 20, 20, 32)        8224      \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 9, 9, 64)          32832     \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 1, 1, 1024)        3212288   \n_________________________________________________________________\nflatten (Flatten)            (None, 1024)              0         \n_________________________________________________________________\ndense (Dense)                (None, 1)                 1025      \n=================================================================\nTotal params: 3,291,297\nTrainable params: 3,291,297\nNon-trainable params: 0\n_________________________________________________________________\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_4 (Conv2D)            (None, 20, 20, 32)        8224      \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 9, 9, 64)          32832     \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 7, 7, 64)          36928     \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 1, 1, 1024)        3212288   \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 1024)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 4)                 4100      \n=================================================================\nTotal params: 3,294,372\nTrainable params: 3,294,372\nNon-trainable params: 0\n_________________________________________________________________\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "class DQN:\n",
    "    \"\"\"Implementation of the brain\"\"\"\n",
    "    def __init__(self, n_actions, stream,learning_rate = 0.00025, state_length = 4):\n",
    "        \"\"\"\n",
    "        Init all required variables for class\n",
    "        :param n_actions      : Number of actions available in the environment \n",
    "        :param stream         : Type of stream - 'v' or 'a' for Advantage and Value respectively\n",
    "        :param learning_rate  : The specific lr argument for the Adam optimizer\n",
    "        :param state_length   : The number of frames which create a state\n",
    "        \"\"\"\n",
    "        self.n_actions = n_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.agent_history_length = state_length\n",
    "        self.stream = stream\n",
    "        self.input_shape = [84, 84, self.agent_history_length]\n",
    "        self.make_model()\n",
    "        \n",
    "    def make_model(self):\n",
    "        \"\"\"\n",
    "        Creates the neural network responsible\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=[8,8], strides=4,\n",
    "                                   input_shape=self.input_shape, \n",
    "                                   activation='relu',\n",
    "                                   kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2)),\n",
    "            tf.keras.layers.Conv2D(filters=64, kernel_size=[4,4], strides=2,\n",
    "                                   input_shape=self.input_shape, \n",
    "                                   activation='relu',\n",
    "                                   kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2)),\n",
    "            tf.keras.layers.Conv2D(filters=64, kernel_size=[3,3], strides=1,\n",
    "                                   input_shape=self.input_shape, \n",
    "                                   activation='relu',\n",
    "                                   kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2)),\n",
    "            tf.keras.layers.Conv2D(filters=1024, kernel_size=[7,7], strides=1,\n",
    "                                   input_shape=self.input_shape, \n",
    "                                   activation='relu',\n",
    "                                   kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2)),\n",
    "            tf.keras.layers.Flatten()\n",
    "        ])\n",
    "        if self.stream == 'v':\n",
    "            self.model.add(\n",
    "                tf.keras.layers.Dense(units=1,\n",
    "                                      kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        elif self.stream == 'a':\n",
    "            self.model.add(\n",
    "                tf.keras.layers.Dense(units=self.n_actions,\n",
    "                                      kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "            \n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate),\n",
    "                           loss=tf.keras.losses.mean_squared_error,\n",
    "                           metrics=['accuracy'])\n",
    "        self.model.summary()\n",
    "Value = DQN(4, 'v')\n",
    "Advantage = DQN(4, 'a')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Making the Get Action Class\n",
    "\n",
    "For the first 50000 frames the agent only explores (Ïµ=1). Over the following 1 million frames, Ïµ is linearly\n",
    "decreased to 0.1\n",
    "\n",
    "After that we chose to decrease it to Ïµ=0.01 over the remaining frames as suggested by the OpenAi Baselines for DQN.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class GetAction:\n",
    "    \"\"\"To implement the Epsilon Greedy Policy for returning actions\"\"\"\n",
    "    def __init__(self, n_actions, eps_initial=1, eps_middle=0.1, eps_final=0.01, eps_eval=0,\n",
    "                 train_start=50e3, train_middle=1e6, train_max=25e6):\n",
    "        \"\"\"\n",
    "        Init all required variables for class\n",
    "        :param n_actions     : Action Space for the environment\n",
    "        :param eps_initial   : Start  Value for Epsilon\n",
    "        :param eps_middle    : Middle Value for Epsilon\n",
    "        :param eps_final     : Final  Value for Epsilon\n",
    "        :param eps_eval      : Epsilon Value to be used during Evaluation\n",
    "        :param train_start   : Number of frames to be pure exploration ie. Epsilon = 1\n",
    "        :param train_middle  : Number of frames for epsilon to decay from eps_initial to eps_middle \n",
    "        :param train_max     : Number of frames for epsilon to decay from eps_middle to eps_final\n",
    "        \"\"\"\n",
    "        self.n_actions = n_actions\n",
    "        self.eps_initial = eps_initial\n",
    "        self.eps_middle = eps_middle\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_eval = eps_eval\n",
    "        self.train_start = train_start\n",
    "        self.train_middle = train_middle\n",
    "        self.train_max = train_max\n",
    "        \n",
    "        # Now we need to generate linear lines for the decay between the start, middle, and the final points\n",
    "        # m = slope = y2-y1 / x2-x1\n",
    "        # c = intercept = y2 - m*x2\n",
    "        self.m_1 = (self.eps_middle - self.eps_initial) / (self.train_middle - self.train_start)\n",
    "        self.c_1 = self.eps_middle - self.m_1*self.train_middle\n",
    "        self.m_2 = (self.eps_final - self.eps_final) / (self.train_max - self.train_middle)\n",
    "        self.c_2 = self.eps_final - self.m_2*self.train_max\n",
    "    \n",
    "    def get_action(self, frame_number, state, dqn_object, evaluation=False):\n",
    "        \"\"\"\n",
    "        Returns the action based on the epsilon greedy policy for the given state\n",
    "        :param frame_number  : Number of frames passed. Used to determine if to use (m_1,c_1) or (m_2,c_2)\n",
    "        :param state         : A stack of frames of the game-play after preprocessing ie. (84,84,4)\n",
    "        :param dqn_object    : DQN object to return the best action\n",
    "        :param evaluation    : Flag to be set True while evaluation. Relies only upon the exploitation\n",
    "        :return: An integer between 0 and n_actions-1 to be set as the action\n",
    "        \"\"\"\n",
    "        if evaluation:\n",
    "            eps = self.eps_eval\n",
    "        elif frame_number < self.train_start:\n",
    "            eps = self.eps_initial\n",
    "        elif frame_number < self.train_middle:\n",
    "            eps = self.m_1*frame_number + self.c_1\n",
    "        elif frame_number < self.train_max:\n",
    "            eps = self.m_2*frame_number + self.c_2\n",
    "        \n",
    "        if np.random.random() < eps:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            return dqn_object.predcit(state)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experience Replay!!\n",
    "\n",
    "> Second, learning directly from consecutive samples is inefficient, due to the strong correlations between \n",
    "the samples; randomizing the samples breaks these correlations and therefore reduces the variance of the updates. \n",
    "Third, when learning on-policy the current parameters determine the next data sample that the parameters are \n",
    "trained on. For example, if the maximizing action is to move left then the training samples will be dominated \n",
    "by samples from the left-hand side; if the maximizing action then switches to the right then the training \n",
    "distribution will also switch. It is easy to see how unwanted feedback loops may arise and the parameters \n",
    "could get stuck in a poor local minimum, or even diverge catastrophically. <br> [Mnih et al. 2013](https://arxiv.org/abs/1312.5602)\n",
    "\n",
    "We add the experiences to a memory holding *deque*. The experiences are of the form - $(state,action,reward,next\\_state,done)$.\n",
    "We store the last 1 million experiences. We fill up the deque with our experiences and sample a batch of random \n",
    "experiences when needed to train. The deque is repopulated as more and more experiences are seen and older ones are\n",
    "removed.\n",
    "\n",
    "Lets make the class. We will sample from a memory buffer of 1 Million"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class ExpMemory:\n",
    "    \"\"\"Storage for the experiences encountered . Stores the last 1 Million experiences\"\"\"\n",
    "    def __init__(self, size=1e6, rows=84, cols=84, state_length=4, batch=32):\n",
    "        \"\"\"\n",
    "        Init all required variables for class\n",
    "        :param size          : The number of experiences to be stored  \n",
    "        :param rows          : The pixel ht of a frame\n",
    "        :param cols          : The pixel wd of a frame\n",
    "        :param state_length  : The number of frames stacked to form a state\n",
    "        :param batch         : The size of a batch to train upon\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.state_length = state_length\n",
    "        self.batch = batch\n",
    "        self.count = 0      # Holds the number of experiences in the deque\n",
    "        self.current = 0    # Holds the index of the last updated experience\n",
    "        # Memory of experiences made:\n",
    "        self.actions = np.empty(self.size, dtype=np.int)                                # Placeholder for actions\n",
    "        self.rewards = np.empty(self.size, dtype=np.float)                              # Placeholder for rewards                \n",
    "        self.dones = np.empty(self.size, dtype=np.bool)                                 # Placeholder for done\n",
    "        self.frames = np.empty((self.size,self.rows,self.cols))                         # Placeholder for frames\n",
    "        self.states = np.empty((self.size,self.state_length,self.rows,self.cols))       # Placeholder for stacked frames\n",
    "        self.new_states = np.empty((self.size,self.state_length,self.rows,self.cols))   # Placeholder for next stacked frames\n",
    "        self.indices = np.empty(self.batch, dtype=np.int)                               # Placeholder for sample of experience tuples\n",
    "    def remember_state(self, action, frame, reward, done):\n",
    "        \"\"\"\n",
    "        Adds the tuple of experience to the memory buffer\n",
    "        :param action   : The action taken during the respective state \n",
    "        :param frame    : Preprocessed game-play frame (84,84,1)\n",
    "        :param reward   : Reward associated with action\n",
    "        :param done     : Done flag returned from taking the step\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if frame.shape != (self.rows,self.cols):\n",
    "            raise ValueError(f\"Wrong dimensions of frame. Reqd - {self.rows},{self.cols} \\t Passed - {frame.shape}\")\n",
    "        self.actions[self.current] = action             \n",
    "        self.frames[self.current, ...] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.dones[self.current] = done\n",
    "        self.current = (self.current+1) % self.size     # Implementation of DeQue on a list\n",
    "        self.count = max(self.current,self.count)      \n",
    "    def _get_state(self, index):\n",
    "        \"\"\"\n",
    "        Gets the corresponding stacked state for the associated index \n",
    "        :param index : The index query  \n",
    "        :return: A list of stacked frames of shape (4,84,84)\n",
    "        \"\"\"\n",
    "        if self.count>0 and index>self.state_length-1:\n",
    "            return self.frames[index-self.state_length+1 : index+1]\n",
    "        else:\n",
    "            raise ValueError(\"Minimum index of 3 is required. Populate the memory buffer\")\n",
    "    def _get_indices(self):\n",
    "        \"\"\"\n",
    "        Gets a list of indices which are valid for experience replay training\n",
    "        :return: A list of size (batch,)\n",
    "        \"\"\"\n",
    "        for i in range(self.batch):\n",
    "            while True:\n",
    "                index = np.random.randint(low=self.state_length, high=self.count)   # Select a random index\n",
    "                if index >= self.current >= index-self.state_length:                # State frames aren't continuous \n",
    "                    continue                                                        # ie. belong to two different times\n",
    "                if self.dones[index-self.state_length : index].any():               # If any of the frames before were part of a different life\n",
    "                    continue\n",
    "                break\n",
    "            self.indices[i] = index                                                 # Valid cases\n",
    "    def get_experience(self):\n",
    "        \"\"\"\n",
    "        Gets a batch of experience tuples of size self.batch\n",
    "        :return: states, actions, rewards, next_state, dones  with self.batch=32 replays\n",
    "        \"\"\"\n",
    "        if self.count<self.state_length:\n",
    "            raise ValueError(\"Too few experiences to get s batch\")\n",
    "        self._get_indices()\n",
    "        for i, index in enumerate(self.indices):\n",
    "            self.states[i] = self._get_state(index)\n",
    "            self.new_states[i] = self._get_state(index)\n",
    "        # We now use transpose just to get the states in the form of (32,84,84,4) -> (batch,rows,cols,state_length)\n",
    "        return np.transpose(self.states, axes=(0,2,3,1)), self.actions[self.indices], self.rewards[self.indices], \\\n",
    "               np.transpose(self.new_states, axes=(0,2,3,1)), self.rewards[self.indices]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Learning Part\n",
    "The problem is that both $Qprediction$ and $Qtarget$ depend on the same parameters $Î¸$ if only one network is used. This \n",
    "can lead to instability when regressing $Qprediction$ towards $Qtarget$ because the \"target is moving\". We ensure a \n",
    "\"fixed target\" by introducing a second network with fixed and only occasionally updated parameters that estimates the \n",
    "target Q-values.\n",
    "> Reinforcement learning is known to be unstable or even to diverge when a nonlinear function approximator such as a \n",
    "neural network is used to represent the action-value (also known as Q) function. This instability has several causes: \n",
    "the correlations present in the sequence of observations, the fact that small updates to Q may significantly change \n",
    "the policy and therefore change the data distribution, and the correlations between the action-values [...] and the \n",
    "target values [...]. We address these instabilities with a novel variant of Q-learning, which uses two key ideas. \n",
    "First, we used a biologically inspired mechanism termed experience replay that randomizes over the data, thereby \n",
    "removing correlations in the observation sequence and smoothing over changes in the data distribution [...]. \n",
    "Second, we used an iterative update that adjusts the action-values (Q) towards target values that are only \n",
    "periodically updated, thereby reducing correlations with the target.\n",
    "[Mnih et al. 2015](https://www.nature.com/articles/nature14236/)\n",
    "\n",
    "One network is used to predict the $Qprediction value while the other network is used to predict the $Qtarget$ value\n",
    "and is fixed. The main network is updated by gradient descent while every 10000 steps the prediction network value is \n",
    "copied to the target network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def learn(exp_memory, main_dqn, target_dqn, batch, gamma):\n",
    "    \"\"\"\n",
    "    Implements the DQN equation\n",
    "    :param exp_memory   : An object of ExpMemory\n",
    "    :param main_dqn     : An object of DQN\n",
    "    :param tsrget_dqn   : An object of DQN\n",
    "    :param batch        : A batch size to perform learning on\n",
    "    :param gamma        : Discounter\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    states, actions, rewards, next_states, dones = exp_memory.get_experience()\n",
    "    for i in range(batch):\n",
    "        next_best_action = np.argmax(main_dqn.predict(next_states[i]))  # Best action in the next Q Table tuple - mainDQN\n",
    "        next_q = target_dqn.predict(next_states[i])                     # Q Table tuple - targetDQN\n",
    "        double_q = next_q[next_best_action]        \n",
    "        target_q = rewards[i] + (1-dones[i])*(gamma*double_q)\n",
    "        main_dqn.fit(states[i], target_q)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}